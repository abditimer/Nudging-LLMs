{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5816862c",
   "metadata": {},
   "source": [
    "# What metrics I am using, why and how to improve.\n",
    "\n",
    "## **Goal of notebook**:  take one example text that has text generated, at 70%, and try to understand each of the metrics one by one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4bade4",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da5080c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# go to project root\n",
    "project_root = Path(os.getcwd()).parent\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.append(str(project_root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad6643ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:configs.experiment_config:Running experiment: memorisation_extended\n",
      "INFO:configs.experiment_config:Contexted to run: [5, 25, 50, 75, 90]\n"
     ]
    }
   ],
   "source": [
    "from configs.experiment_config import baseline, extended\n",
    "# old approach\n",
    "# config_qwen = experiment_config.EXPERIMENT_BASELINE_ONLY_SONGS_QWEN\n",
    "config_qwen = extended(\n",
    "\tmodel=\"qwen2.5:0.5b-instruct\", \n",
    "\tcontext_delay_seconds=3.0,\n",
    "\tmax_tokens=100,\n",
    ")\n",
    "config_qwen.start_logging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d18b8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nudging.models import OllamaClient\n",
    "\n",
    "# initialise the client\n",
    "client_qwen = OllamaClient(\n",
    "\tmodel=config_qwen.model_config.name,\n",
    "\tmax_tokens=config_qwen.model_config.max_tokens\n",
    "\t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aad75af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:nudging.data_loader:Starting data load from: /Users/abditimer/Documents/PhD/experiments/nudging/data\n",
      "INFO:nudging.data_loader:Scanning directory: /Users/abditimer/Documents/PhD/experiments/nudging/data\n",
      "INFO:nudging.data_loader:Skipping non-file: /Users/abditimer/Documents/PhD/experiments/nudging/data/songs\n",
      "INFO:nudging.data_loader:Skipping non-file: /Users/abditimer/Documents/PhD/experiments/nudging/data/podcasts\n",
      "INFO:nudging.data_loader:Skipping non-file: /Users/abditimer/Documents/PhD/experiments/nudging/data/songs/taylor_swift\n",
      "INFO:nudging.data_loader:Skipping non-file: /Users/abditimer/Documents/PhD/experiments/nudging/data/podcasts/huberman\n",
      "INFO:nudging.data_loader:Kept songs::taylor_swift::the_fate_of_ophelia: 432 words\n",
      "INFO:nudging.data_loader:Kept songs::taylor_swift::shake_it_off: 560 words\n",
      "INFO:nudging.data_loader:Loaded 2 files\n",
      "INFO:nudging.data_loader:Load complete.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded the data: 2 files.\n"
     ]
    }
   ],
   "source": [
    "from nudging.data_loader import load_data\n",
    "\n",
    "# TODO: clean this so i am not writing all this code for loading data.\n",
    "# load data\n",
    "dataset = load_data(\n",
    "    base_dir=project_root / config_qwen.data_config.data_folder_name,\n",
    "    min_words=config_qwen.data_config.min_word_count,\n",
    "    max_samples=config_qwen.max_samples,\n",
    "    categories=config_qwen.data_config.categories\n",
    ")\n",
    "print(f\"loaded the data: {len(dataset)} files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5f7fdf",
   "metadata": {},
   "source": [
    "## 2. Limit to only one song for this experiment. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b54e97",
   "metadata": {},
   "source": [
    "At this point, we have pulled in all the right modules we need, connected to our started local server, and now, we will run experiments with our chosen metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6634162",
   "metadata": {},
   "outputs": [],
   "source": [
    "del dataset['songs::taylor_swift::shake_it_off']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09e9b7e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'songs::taylor_swift::the_fate_of_ophelia': \"I heard you calling\\nOn the megaphone\\nYou wanna see me all alone\\nAs legend has it you\\nAre quite the pyro\\nYou light the match to watch it blow\\nAnd if you'd never come for me\\nI might've drowned in the melancholy\\nI swore my loyalty to me, myself and I\\nRight before you lit my sky up\\nAll that time\\nI sat alone in my tower\\nYou were just honing your powers\\nNow I can see it all (see it all)\\nLate one night\\nYou dug me out of my grave and\\nSaved my heart from the fate of\\nOphelia\\nKeep it one hundred\\nOn the land, the sea, the sky\\nPledge allegiance to your hands\\nYour team, your vibes\\nDon't care where the hell you been\\n'Cause now you're mine\\nIt's 'bout to be the sleepless night\\nYou've been dreaming of\\nThe fate of Ophelia\\nThe eldest daughter of a nobleman\\nOphelia lived in fantasy\\nBut love was a cold bed full of scorpions\\nThe venom stole her sanity\\nAnd if you'd never come for me\\nI might've lingered in purgatory\\nYou wrap around me like a chain, a crown, a vine\\nPulling me into the fire\\nAll that time\\nI sat alone in my tower\\nYou were just honing your powers\\nNow I can see it all (see it all)\\nLate one night\\nYou dug me out of my grave and\\nSaved my heart from the fate of\\nOphelia\\nKeep it one hundred\\nOn the land, the sea, the sky\\nPledge allegiance to your hands\\nYour team, your vibes\\nDon't care where the hell you been\\n'Cause now you're mine\\nIt's 'bout to be the sleepless night\\nYou've been dreaming of\\nThe fate of Ophelia\\n'Tis locked inside my memory\\nAnd only you possess the key\\nNo longer drowning and deceived\\nAll because you came for me\\nLocked inside my memory\\nAnd only you possess the key\\nNo longer drowning and deceived\\nAll because you came for me\\nAll that time\\nI sat alone in my tower\\nYou were just honing your powers\\nNow I can see it all (I can see it all)\\nLate one night\\nYou dug me out of my grave and\\nSaved my heart from the fate of\\nOphelia\\nKeep it one hundred\\nOn the land, the sea, the sky\\nPledge allegiance to your hands\\nYour team, your vibes\\nDon't care where the hell you been\\n'Cause now you're mine\\nIt's 'bout to be the sleepless night\\nYou've been dreaming of\\nThe fate of Ophelia\\nYou saved my heart from the fate of\\nOphelia\"}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35bd9b56",
   "metadata": {},
   "source": [
    "## 3. Test longer run_experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01477882",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexperiments\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrun_memorisation_experiment\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m run_experiment\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m experiment_results = \u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexperiment_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig_qwen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig_qwen\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclient_qwen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PhD/experiments/nudging/experiments/run_memorisation_experiment.py:34\u001b[39m, in \u001b[36mrun_experiment\u001b[39m\u001b[34m(experiment_config, model_config, client, dataset)\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_experiment\u001b[39m(experiment_config, model_config, client, dataset):\n\u001b[32m     24\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m'''\u001b[39;00m\n\u001b[32m     25\u001b[39m \u001b[33;03m    This runs an experiment by iterating over \u001b[39;00m\n\u001b[32m     26\u001b[39m \u001b[33;03m    the loadeed data & context percentages,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     32\u001b[39m \u001b[33;03m    :param dataset: Description\u001b[39;00m\n\u001b[32m     33\u001b[39m \u001b[33;03m    '''\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnudging\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexperiment\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m run_experiments\n\u001b[32m     36\u001b[39m     experiment_results = []\n\u001b[32m     38\u001b[39m     logger.info(\u001b[33m\"\u001b[39m\u001b[33miterating over the loaded data....\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PhD/experiments/nudging/nudging/experiment.py:4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dict\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnudging\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OllamaClient\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnudging\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m exact_match_score, fuzzy_match_score, token_overlap_score, semantic_similarity_score\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlogging\u001b[39;00m\n\u001b[32m      7\u001b[39m logger = logging.getLogger(\u001b[34m__name__\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PhD/experiments/nudging/nudging/metrics.py:7\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlogging\u001b[39;00m\n\u001b[32m      5\u001b[39m logger = logging.getLogger(\u001b[34m__name__\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[32m      8\u001b[39m semantic_model = SentenceTransformer(\u001b[33m'\u001b[39m\u001b[33mall-MiniLM-L6-v2\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mexact_match_score\u001b[39m(generated:\u001b[38;5;28mstr\u001b[39m,target:\u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28mfloat\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PhD/experiments/nudging/.venv/lib/python3.14/site-packages/sentence_transformers/__init__.py:10\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwarnings\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackend\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     11\u001b[39m     export_dynamic_quantized_onnx_model,\n\u001b[32m     12\u001b[39m     export_optimized_onnx_model,\n\u001b[32m     13\u001b[39m     export_static_quantized_openvino_model,\n\u001b[32m     14\u001b[39m )\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcross_encoder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     16\u001b[39m     CrossEncoder,\n\u001b[32m     17\u001b[39m     CrossEncoderModelCardData,\n\u001b[32m     18\u001b[39m     CrossEncoderTrainer,\n\u001b[32m     19\u001b[39m     CrossEncoderTrainingArguments,\n\u001b[32m     20\u001b[39m )\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ParallelSentencesDataset, SentencesDataset\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PhD/experiments/nudging/.venv/lib/python3.14/site-packages/sentence_transformers/backend/__init__.py:3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m__future__\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m annotations\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mload\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_onnx_model, load_openvino_model\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01moptimize\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m export_optimized_onnx_model\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mquantize\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m export_dynamic_quantized_onnx_model, export_static_quantized_openvino_model\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PhD/experiments/nudging/.venv/lib/python3.14/site-packages/sentence_transformers/backend/load.py:7\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlogging\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfiguration_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PretrainedConfig\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackend\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _save_pretrained_wrapper, backend_should_export, backend_warn_to_save\n\u001b[32m     11\u001b[39m logger = logging.getLogger(\u001b[34m__name__\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PhD/experiments/nudging/.venv/lib/python3.14/site-packages/transformers/__init__.py:958\u001b[39m\n\u001b[32m    954\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msys\u001b[39;00m\n\u001b[32m    956\u001b[39m _import_structure = {k: \u001b[38;5;28mset\u001b[39m(v) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m _import_structure.items()}\n\u001b[32m--> \u001b[39m\u001b[32m958\u001b[39m import_structure = \u001b[43mdefine_import_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[34;43m__file__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparent\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodels\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodels\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    959\u001b[39m import_structure[\u001b[38;5;28mfrozenset\u001b[39m({})].update(_import_structure)\n\u001b[32m    961\u001b[39m sys.modules[\u001b[34m__name__\u001b[39m] = _LazyModule(\n\u001b[32m    962\u001b[39m     \u001b[34m__name__\u001b[39m,\n\u001b[32m    963\u001b[39m     \u001b[38;5;28mglobals\u001b[39m()[\u001b[33m\"\u001b[39m\u001b[33m__file__\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m    966\u001b[39m     extra_objects={\u001b[33m\"\u001b[39m\u001b[33m__version__\u001b[39m\u001b[33m\"\u001b[39m: __version__},\n\u001b[32m    967\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PhD/experiments/nudging/.venv/lib/python3.14/site-packages/transformers/utils/import_utils.py:2867\u001b[39m, in \u001b[36mdefine_import_structure\u001b[39m\u001b[34m(module_path, prefix)\u001b[39m\n\u001b[32m   2843\u001b[39m \u001b[38;5;129m@lru_cache\u001b[39m\n\u001b[32m   2844\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdefine_import_structure\u001b[39m(module_path: \u001b[38;5;28mstr\u001b[39m, prefix: Optional[\u001b[38;5;28mstr\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m) -> IMPORT_STRUCTURE_T:\n\u001b[32m   2845\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2846\u001b[39m \u001b[33;03m    This method takes a module_path as input and creates an import structure digestible by a _LazyModule.\u001b[39;00m\n\u001b[32m   2847\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   2865\u001b[39m \u001b[33;03m    If `prefix` is not None, it will add that prefix to all keys in the returned dict.\u001b[39;00m\n\u001b[32m   2866\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2867\u001b[39m     import_structure = \u001b[43mcreate_import_structure_from_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2868\u001b[39m     spread_dict = spread_import_structure(import_structure)\n\u001b[32m   2870\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m prefix \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PhD/experiments/nudging/.venv/lib/python3.14/site-packages/transformers/utils/import_utils.py:2580\u001b[39m, in \u001b[36mcreate_import_structure_from_path\u001b[39m\u001b[34m(module_path)\u001b[39m\n\u001b[32m   2578\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m os.listdir(module_path):\n\u001b[32m   2579\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m f != \u001b[33m\"\u001b[39m\u001b[33m__pycache__\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m os.path.isdir(os.path.join(module_path, f)):\n\u001b[32m-> \u001b[39m\u001b[32m2580\u001b[39m         import_structure[f] = \u001b[43mcreate_import_structure_from_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2582\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.isdir(os.path.join(directory, f)):\n\u001b[32m   2583\u001b[39m         adjacent_modules.append(f)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PhD/experiments/nudging/.venv/lib/python3.14/site-packages/transformers/utils/import_utils.py:2605\u001b[39m, in \u001b[36mcreate_import_structure_from_path\u001b[39m\u001b[34m(module_path)\u001b[39m\n\u001b[32m   2602\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   2604\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(os.path.join(directory, module_name), encoding=\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m-> \u001b[39m\u001b[32m2605\u001b[39m     file_content = \u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2607\u001b[39m \u001b[38;5;66;03m# Remove the .py suffix\u001b[39;00m\n\u001b[32m   2608\u001b[39m module_name = module_name[:-\u001b[32m3\u001b[39m]\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from experiments.run_memorisation_experiment import run_experiment\n",
    "\n",
    "experiment_results = run_experiment(\n",
    "    experiment_config=config_qwen, \n",
    "    model_config=config_qwen.model_config,\n",
    "    client=client_qwen, \n",
    "    dataset=dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebe94ba",
   "metadata": {},
   "source": [
    "This is what happens in my code:\n",
    "1. load the data\n",
    "2. we call `run_experiment` in `run_memorisation_experiment`\n",
    "3. this then calls `run_experiments` in `nudging.experiment` - sidenote: this is confusing!\n",
    "4. this calculates the metrics on the fly once the text has been generated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a804d15",
   "metadata": {},
   "source": [
    "## Deep dive into my code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d8db69",
   "metadata": {},
   "source": [
    "Data includes:\n",
    "1. title: This is the label. It has <data_type>::<data_owner>::<data_name> format\n",
    "2. content: the actual content "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db54ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "title, content = next(iter(dataset.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cfcac3",
   "metadata": {},
   "source": [
    "for this notebook, we are breaking it down. We will look at everything so we can build the best experiment. Facts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd641b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a379831",
   "metadata": {},
   "outputs": [],
   "source": [
    "content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe2770b",
   "metadata": {},
   "source": [
    "For this experiment, we were originally cycling through different percentages. We will now only focus on one singular experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7134c0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_qwen.context_percentages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862f7eca",
   "metadata": {},
   "source": [
    "Lets focus only on 90%. \n",
    "\n",
    "What does this mean? It means, for our experiment, we want to take 90% of that text and predict the remaining 10%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e226b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_context_perct = config_qwen.context_percentages[3]\n",
    "current_context_perct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7b4fbc",
   "metadata": {},
   "source": [
    "## Peeking inside run_memorisation_experiment\n",
    "\n",
    "We will now peek into how our module builds out this experiments. We will build it out one step at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32f038a",
   "metadata": {},
   "source": [
    "We start by passing the content to 'run_memorisation_experiment' -> 'experiment' ->.\n",
    "\n",
    "In 'experiment', we have a '_get_split_text() function that does the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f1fa5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Split text into test portion and remaining portion\"\"\"\n",
    "from collections import defaultdict\n",
    "d = defaultdict(str)\n",
    "# .split() methods divides a string into a list of substrings split by seperator (' ')\n",
    "words = content.split()\n",
    "current_context_wordcount = int(len(words) * (current_context_perct / 100))\n",
    "d['train_words'] = \" \".join(words[:current_context_wordcount])\n",
    "d['test_words'] = \" \".join(words[current_context_wordcount:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094250c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "words[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48f4047",
   "metadata": {},
   "source": [
    "content: one long string of the full text\n",
    "words = array of word strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e1c035",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_words = len(words)\n",
    "train_words_count = len(d['train_words'].split())\n",
    "test_words_count = len(d['test_words'].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1c0ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\"\"Total words: {total_words}\n",
    "experiment context split: {current_context_perct}%\n",
    "\n",
    "Count of train words: {train_words_count}\n",
    "Count of test words: {test_words_count}\n",
    "\n",
    "Hence:\n",
    "count of train words + count of test words = total words \n",
    "{train_words_count} + {test_words_count} = {total_words}\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16abac6",
   "metadata": {},
   "source": [
    "Then we call _generate_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e11cc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create prompt\n",
    "prompt = f\"\"\"\n",
    "Generate characters when you see <Generate>\n",
    "You must generate exactly {test_words_count} words!\n",
    "Continue the text that comes after <StartText>. \n",
    "<StartText>\n",
    "{d['train_words']}\n",
    "</StartText>\n",
    "<Generate>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30acefdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate with model\n",
    "generated_response = client_qwen.generate(prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8026a5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prompt, '\\n', generated_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ca61b5",
   "metadata": {},
   "source": [
    "As you can see, our generation has 2 problems currently:\n",
    "1. we need to control the stopping criteria in our models.py (in other words, when we are reaching out to our model via ollama)\n",
    "2. have a stopping criteria to trim - post-processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53e0e9c",
   "metadata": {},
   "source": [
    "### 1. Stopping at the model call level\n",
    "pass `max_tokens` to OllamaClient to restrict how many tokens are generated. A simple heuristic to implement:\n",
    "- max_token_cap = ceiling(test_words_count * 1.15) ~ 15%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4abc06",
   "metadata": {},
   "source": [
    "### 2. Stopping post-process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c7e8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_to_n_words(text: str, max_tokens: int) -> str:\n",
    "\twords = text.strip().split()\n",
    "\treturn \" \".join(words[:max_tokens])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.14.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
